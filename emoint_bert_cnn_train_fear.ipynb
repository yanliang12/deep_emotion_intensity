{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"sadness\"\n",
    "emotion = \"joy\"\n",
    "emotion = \"anger\"\n",
    "\n",
    "emotion = \"fear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aaad343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"E:\\data\\emoint\"\n",
    "#data_file = \"E:\\\\data\\\\emoint\\\\tweets_sm.xlsx\"\n",
    "data_file = \"E:\\\\data\\\\emoint\\\\tweets_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import keras\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d697470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.utils import *\n",
    "from keras.models import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81906000",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 100 # max number of words in one tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b673e5",
   "metadata": {},
   "source": [
    "df_list = []\n",
    "\n",
    "for f in os.listdir(data_folder):\n",
    "    if bool(re.search(r'\\.txt$', f)):\n",
    "        f1 = '{}\\{}'.format(data_folder, f)\n",
    "        df = pd.read_csv(f1, \n",
    "            header = 0,\n",
    "            names = ['tweet_id', 'text', 'emotion', 'intensity'],\n",
    "            delimiter = '\\t',\n",
    "            )\n",
    "        df_list.append(df)\n",
    "\n",
    "df_all = pd.concat(df_list)\n",
    "\n",
    "#df_all = df_all.head(1000)\n",
    "\n",
    "df_all.to_excel(\n",
    "    '{}/tweets_all.xlsx'.format(data_folder),\n",
    "    index = False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f808985",
   "metadata": {},
   "source": [
    "# data conversion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6b793",
   "metadata": {},
   "source": [
    "## function to convert text file to text and tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9f8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_tag_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data['label'] = data['emotion'].apply(lambda x: 1 if x == emotion else 0)\n",
    "    texts = data['text'].to_list()\n",
    "    tags = data['label'].to_list()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09973d42",
   "metadata": {},
   "source": [
    "## function to convert text file to text and intensentiy score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edf8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(\n",
    "    x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_file_to_text_and_score_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data = data[data['emotion'] == emotion]\n",
    "    data['intensity'] = data['intensity'].apply(str_to_float)\n",
    "    data = data[data.intensity.notnull()]\n",
    "    texts = data['text'].to_list()\n",
    "    scores = data['intensity'].to_list()\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3376",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824ba35",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9482ac5",
   "metadata": {},
   "source": [
    "## convert text list to a input format of deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daefbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee23928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_input(\n",
    "    texts,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors=\"tf\",\n",
    "        )\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef5720",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb2ce8",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"], method = 'one_hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b004",
   "metadata": {},
   "source": [
    "## function of building the model of text emotion tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dropout_rate = 0.2\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_tagger_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(2, \n",
    "        activation=\"softmax\",\n",
    "         name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf77a",
   "metadata": {},
   "source": [
    "## function of train the model of emotion tagger with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedc6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(\n",
    "    texts,\n",
    "    tags,\n",
    "    tagger_model_path = None,\n",
    "    tagger_model_weight_path = None,\n",
    "    tagger_model_json_path = None,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    dropout_rate = 0.2,\n",
    "    ):\n",
    "    tagger_model = emotion_tagger_model_building(\n",
    "        dropout_rate = dropout_rate,\n",
    "        )\n",
    "    '''\n",
    "    prepare the text input\n",
    "\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    print(numpy.sum(y, axis = 0))\n",
    "    # Fit the model using the train and test datasets.\n",
    "    tagger_model.fit(\n",
    "        [x_ids, x_attention, x_type], y, \n",
    "        validation_split=validation_split, \n",
    "        epochs=epochs)\n",
    "    if tagger_model_path is not None:\n",
    "        tagger_model.save(tagger_model_path)\n",
    "    return tagger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052acfb",
   "metadata": {},
   "source": [
    "# function of intensity score model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee904c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scorer_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):   \n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, \n",
    "        activation=\"sigmoid\",\n",
    "        name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[metrics.mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05725cef",
   "metadata": {},
   "source": [
    "# function of training intensity score model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fea3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scorer(\n",
    "    texts,\n",
    "    scores,\n",
    "    scorer_model_path,\n",
    "    epochs = 10,\n",
    "    validation_split=0.1,\n",
    "    ):\n",
    "    scorer_model = emotion_scorer_model_building()\n",
    "    '''\n",
    "    prepare the text input\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(scores)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    # Fit the model using the train and test datasets.\n",
    "    scorer_model.fit(\n",
    "        [x_ids, x_attention, x_type], \n",
    "        y, \n",
    "        validation_split=0.1, \n",
    "        epochs=epochs)\n",
    "    scorer_model.save(\n",
    "        scorer_model_path,\n",
    "        save_format='h5')\n",
    "    return scorer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d885",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89f6c1",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa43e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_model_fold_cross_validation(\n",
    "    texts,\n",
    "    tags,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_tagger_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('accuracy of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('accuracy of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fcb15",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_model_fold_cross_validation(\n",
    "    texts,\n",
    "    scores,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(scores)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_scorer_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('mse of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('mse of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc963b1b",
   "metadata": {},
   "source": [
    "# training of each emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6adcff",
   "metadata": {},
   "source": [
    "### tagger training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924f00c",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4b4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_tags = convert_file_to_text_and_tag_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2fa4",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18caf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 546s 1s/step - loss: 0.5440 - accuracy: 0.7284\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 481s 2s/step - loss: 0.3783 - accuracy: 0.8364\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 408s 2s/step - loss: 0.3189 - accuracy: 0.8612\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2838 - accuracy: 0.8798\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 459s 2s/step - loss: 0.2445 - accuracy: 0.9002\n",
      "67/67 [==============================] - 127s 2s/step - loss: 0.2345 - accuracy: 0.9087\n",
      "accuracy of the 1-th fold:0.9087470173835754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 456s 2s/step - loss: 0.5144 - accuracy: 0.7461\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 429s 2s/step - loss: 0.3744 - accuracy: 0.8347\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.3201 - accuracy: 0.8622\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 426s 2s/step - loss: 0.2891 - accuracy: 0.8780\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 561s 2s/step - loss: 0.2513 - accuracy: 0.8977\n",
      "67/67 [==============================] - 130s 2s/step - loss: 0.2535 - accuracy: 0.9011\n",
      "accuracy of the 2-th fold:0.901135265827179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 601s 2s/step - loss: 0.5171 - accuracy: 0.7416\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 2064s 8s/step - loss: 0.3754 - accuracy: 0.8358\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 577s 2s/step - loss: 0.3199 - accuracy: 0.8631\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 628s 2s/step - loss: 0.2826 - accuracy: 0.8764\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 545s 2s/step - loss: 0.2500 - accuracy: 0.8941\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.2346 - accuracy: 0.9026\n",
      "accuracy of the 3-th fold:0.9025543928146362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 456s 2s/step - loss: 0.5311 - accuracy: 0.7418\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 582s 2s/step - loss: 0.3725 - accuracy: 0.8354\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 614s 2s/step - loss: 0.3214 - accuracy: 0.8631\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 608s 2s/step - loss: 0.2822 - accuracy: 0.8813\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 522s 2s/step - loss: 0.2492 - accuracy: 0.8970\n",
      "67/67 [==============================] - 105s 1s/step - loss: 0.2443 - accuracy: 0.9002\n",
      "accuracy of the 4-th fold:0.900189220905304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 512s 2s/step - loss: 0.5305 - accuracy: 0.7324\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 4173s 16s/step - loss: 0.3713 - accuracy: 0.8365\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 575s 2s/step - loss: 0.3271 - accuracy: 0.8598\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 621s 2s/step - loss: 0.2790 - accuracy: 0.8845\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 438s 2s/step - loss: 0.2368 - accuracy: 0.8993\n",
      "67/67 [==============================] - 166s 2s/step - loss: 0.2324 - accuracy: 0.9063\n",
      "accuracy of the 5-th fold:0.9063386917114258\n",
      "accuracy of 5-fold cross validation:\t0.9037929177284241\n"
     ]
    }
   ],
   "source": [
    "tagger_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_tags,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ddd7",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d532dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 2)\n",
      "[7219. 3352.]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 721s 2s/step - loss: 0.5253 - accuracy: 0.7302 - val_loss: 0.5310 - val_accuracy: 0.7637\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 538s 2s/step - loss: 0.3585 - accuracy: 0.8399 - val_loss: 0.4251 - val_accuracy: 0.8270\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 536s 2s/step - loss: 0.3026 - accuracy: 0.8673 - val_loss: 0.3923 - val_accuracy: 0.8497\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 658s 2s/step - loss: 0.2625 - accuracy: 0.8905 - val_loss: 0.2505 - val_accuracy: 0.9178\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 655s 2s/step - loss: 0.2284 - accuracy: 0.9059 - val_loss: 0.4418 - val_accuracy: 0.8488\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 552s 2s/step - loss: 0.2049 - accuracy: 0.9199 - val_loss: 0.3584 - val_accuracy: 0.8828\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 714s 2s/step - loss: 0.1732 - accuracy: 0.9311 - val_loss: 0.5607 - val_accuracy: 0.7958\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 307s 1s/step - loss: 0.1525 - accuracy: 0.9393 - val_loss: 0.3842 - val_accuracy: 0.8762\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 266s 893ms/step - loss: 0.1347 - accuracy: 0.9452 - val_loss: 0.6050 - val_accuracy: 0.7977\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 236s 792ms/step - loss: 0.1219 - accuracy: 0.9525 - val_loss: 0.4892 - val_accuracy: 0.8223\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_tagger(\n",
    "    texts = fear_texts,\n",
    "    tags = fear_tags,\n",
    "    tagger_model_path = '/data/emoint/{}_tagger_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t5203.104001760483\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b5b2",
   "metadata": {},
   "source": [
    "### scorer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc17c0",
   "metadata": {},
   "source": [
    "#### laod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21372afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_scores = convert_file_to_text_and_score_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea020",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ddf9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "57/57 [==============================] - 26s 277ms/step - loss: 0.0416 - mean_absolute_error: 0.1639\n",
      "Epoch 2/5\n",
      "57/57 [==============================] - 16s 287ms/step - loss: 0.0241 - mean_absolute_error: 0.1257\n",
      "Epoch 3/5\n",
      "57/57 [==============================] - 17s 292ms/step - loss: 0.0202 - mean_absolute_error: 0.1122\n",
      "Epoch 4/5\n",
      "57/57 [==============================] - 17s 296ms/step - loss: 0.0179 - mean_absolute_error: 0.1064\n",
      "Epoch 5/5\n",
      "57/57 [==============================] - 17s 293ms/step - loss: 0.0159 - mean_absolute_error: 0.0998\n",
      "15/15 [==============================] - 6s 255ms/step - loss: 0.0165 - mean_absolute_error: 0.1002\n",
      "mse of the 1-th fold:0.10018211603164673\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscorer_model_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfear_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfear_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mscorer_model_fold_cross_validation\u001b[1;34m(texts, scores, n_splits)\u001b[0m\n\u001b[0;32m     15\u001b[0m fold_no \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(x_ids, y):\n\u001b[1;32m---> 17\u001b[0m     tagger_model \u001b[38;5;241m=\u001b[39m \u001b[43memotion_scorer_model_building\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     tagger_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     19\u001b[0m         [x_ids[train], x_attention[train], x_type[train]],\n\u001b[0;32m     20\u001b[0m         y[train], \n\u001b[0;32m     21\u001b[0m         epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     22\u001b[0m         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     scores \u001b[38;5;241m=\u001b[39m tagger_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m     24\u001b[0m         [x_ids[test], x_attention[test], x_type[test]],\n\u001b[0;32m     25\u001b[0m         y[test], \n\u001b[0;32m     26\u001b[0m         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36memotion_scorer_model_building\u001b[1;34m(embedding_dim, filters, kernel_size, dropout_rate, sequence_length)\u001b[0m\n\u001b[0;32m     17\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[0;32m     18\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(sequence_length,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Loading pretrained BERT model.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Freeze the BERT model to reuse the pretrained features without modifying them.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m bert_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1809\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# 'by_name' allow us to do transfer learning by skipping/adding layers\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;66;03m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[39;00m\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1809\u001b[0m     missing_keys, unexpected_keys, mismatched_keys \u001b[38;5;241m=\u001b[39m \u001b[43mload_tf_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:646\u001b[0m, in \u001b[0;36mload_tf_weights\u001b[1;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[0;32m    643\u001b[0m                     weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((symbolic_weight, array))\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# Load all the weights\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_value_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;66;03m# Compute the missing and unexpected layers\u001b[39;00m\n\u001b[0;32m    649\u001b[0m missing_layers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(symbolic_weights_names \u001b[38;5;241m-\u001b[39m saved_weight_names_set))\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    105\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "scorer_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25872af",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_scorer(\n",
    "    texts = fear_texts,\n",
    "    scores = fear_scores,\n",
    "    scorer_model_path = '/data/emoint/{}_scorer_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
