{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"fear\"\n",
    "emotion = \"joy\"\n",
    "emotion = \"anger\"\n",
    "\n",
    "emotion = \"sadness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9072ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"E:\\data\\emoint\"\n",
    "#data_file = \"E:\\\\data\\\\emoint\\\\tweets_sm.xlsx\"\n",
    "data_file = \"E:\\\\data\\\\emoint\\\\tweets_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import keras\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d697470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.utils import *\n",
    "from keras.models import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81906000",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 100 # max number of words in one tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2332eb4",
   "metadata": {},
   "source": [
    "df_list = []\n",
    "\n",
    "for f in os.listdir(data_folder):\n",
    "    if bool(re.search(r'\\.txt$', f)):\n",
    "        f1 = '{}\\{}'.format(data_folder, f)\n",
    "        df = pd.read_csv(f1, \n",
    "            header = 0,\n",
    "            names = ['tweet_id', 'text', 'emotion', 'intensity'],\n",
    "            delimiter = '\\t',\n",
    "            )\n",
    "        df_list.append(df)\n",
    "\n",
    "df_all = pd.concat(df_list)\n",
    "\n",
    "#df_all = df_all.head(1000)\n",
    "\n",
    "df_all.to_excel(\n",
    "    '{}/tweets_all.xlsx'.format(data_folder),\n",
    "    index = False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f808985",
   "metadata": {},
   "source": [
    "# data conversion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6b793",
   "metadata": {},
   "source": [
    "## function to convert text file to text and tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9f8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_tag_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data['label'] = data['emotion'].apply(lambda x: 1 if x == emotion else 0)\n",
    "    texts = data['text'].to_list()\n",
    "    tags = data['label'].to_list()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09973d42",
   "metadata": {},
   "source": [
    "## function to convert text file to text and intensentiy score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edf8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(\n",
    "    x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_file_to_text_and_score_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data = data[data['emotion'] == emotion]\n",
    "    data['intensity'] = data['intensity'].apply(str_to_float)\n",
    "    data = data[data.intensity.notnull()]\n",
    "    texts = data['text'].to_list()\n",
    "    scores = data['intensity'].to_list()\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3376",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824ba35",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9482ac5",
   "metadata": {},
   "source": [
    "## convert text list to a input format of deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daefbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee23928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_input(\n",
    "    texts,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors=\"tf\",\n",
    "        )\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef5720",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb2ce8",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"], method = 'one_hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b004",
   "metadata": {},
   "source": [
    "## function of building the model of text emotion tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dropout_rate = 0.2\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_tagger_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(2, \n",
    "        activation=\"softmax\",\n",
    "         name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf77a",
   "metadata": {},
   "source": [
    "## function of train the model of emotion tagger with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedc6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(\n",
    "    texts,\n",
    "    tags,\n",
    "    tagger_model_path = None,\n",
    "    tagger_model_weight_path = None,\n",
    "    tagger_model_json_path = None,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    dropout_rate = 0.2,\n",
    "    ):\n",
    "    tagger_model = emotion_tagger_model_building(\n",
    "        dropout_rate = dropout_rate,\n",
    "        )\n",
    "    '''\n",
    "    prepare the text input\n",
    "\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    print(numpy.sum(y, axis = 0))\n",
    "    # Fit the model using the train and test datasets.\n",
    "    tagger_model.fit(\n",
    "        [x_ids, x_attention, x_type], y, \n",
    "        validation_split=validation_split, \n",
    "        epochs=epochs)\n",
    "    if tagger_model_path is not None:\n",
    "        tagger_model.save(tagger_model_path)\n",
    "    return tagger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052acfb",
   "metadata": {},
   "source": [
    "# function of intensity score model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee904c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scorer_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):   \n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, \n",
    "        activation=\"sigmoid\",\n",
    "        name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[metrics.mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05725cef",
   "metadata": {},
   "source": [
    "# function of training intensity score model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fea3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scorer(\n",
    "    texts,\n",
    "    scores,\n",
    "    scorer_model_path,\n",
    "    epochs = 10,\n",
    "    validation_split=0.1,\n",
    "    ):\n",
    "    scorer_model = emotion_scorer_model_building()\n",
    "    '''\n",
    "    prepare the text input\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(scores)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    # Fit the model using the train and test datasets.\n",
    "    scorer_model.fit(\n",
    "        [x_ids, x_attention, x_type], \n",
    "        y, \n",
    "        validation_split=0.1, \n",
    "        epochs=epochs)\n",
    "    scorer_model.save(\n",
    "        scorer_model_path,\n",
    "        save_format='h5')\n",
    "    return scorer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d885",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89f6c1",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa43e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_model_fold_cross_validation(\n",
    "    texts,\n",
    "    tags,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_tagger_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('accuracy of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('accuracy of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fcb15",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_model_fold_cross_validation(\n",
    "    texts,\n",
    "    scores,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(scores)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_scorer_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('mse of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('mse of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc963b1b",
   "metadata": {},
   "source": [
    "# training of each emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6adcff",
   "metadata": {},
   "source": [
    "### tagger training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924f00c",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4b4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_tags = convert_file_to_text_and_tag_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2fa4",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18caf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 395s 1s/step - loss: 0.4768 - accuracy: 0.7980\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 420s 2s/step - loss: 0.3343 - accuracy: 0.8628\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 453s 2s/step - loss: 0.2813 - accuracy: 0.8843\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 400s 2s/step - loss: 0.2465 - accuracy: 0.8984\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 414s 2s/step - loss: 0.2041 - accuracy: 0.9192\n",
      "67/67 [==============================] - 99s 1s/step - loss: 0.1777 - accuracy: 0.9338\n",
      "accuracy of the 1-th fold:0.9338061213493347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 410s 1s/step - loss: 0.4576 - accuracy: 0.8076\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 428s 2s/step - loss: 0.3257 - accuracy: 0.8672\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2741 - accuracy: 0.8870\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2195 - accuracy: 0.9114\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 420s 2s/step - loss: 0.2055 - accuracy: 0.9181\n",
      "67/67 [==============================] - 97s 1s/step - loss: 0.1880 - accuracy: 0.9262\n",
      "accuracy of the 2-th fold:0.9262062311172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 405s 1s/step - loss: 0.4545 - accuracy: 0.8094\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 385s 1s/step - loss: 0.3215 - accuracy: 0.8684\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 386s 1s/step - loss: 0.2661 - accuracy: 0.8932\n",
      "Epoch 4/5\n",
      " 44/265 [===>..........................] - ETA: 20:07 - loss: 0.2390 - accuracy: 0.9070"
     ]
    }
   ],
   "source": [
    "tagger_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_tags,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ddd7",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_tagger(\n",
    "    texts = fear_texts,\n",
    "    tags = fear_tags,\n",
    "    tagger_model_path = '/data/emoint/{}_tagger_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968de145",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b5b2",
   "metadata": {},
   "source": [
    "### scorer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc17c0",
   "metadata": {},
   "source": [
    "#### laod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21372afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_scores = convert_file_to_text_and_score_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea020",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25872af",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_scorer(\n",
    "    texts = fear_texts,\n",
    "    scores = fear_scores,\n",
    "    scorer_model_path = '/data/emoint/{}_scorer_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
