{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"fear\"\n",
    "emotion = \"joy\"\n",
    "emotion = \"anger\"\n",
    "\n",
    "emotion = \"sadness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9072ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"E:\\data\\emoint\"\n",
    "#data_file = \"E:\\\\data\\\\emoint\\\\tweets_sm.xlsx\"\n",
    "data_file = \"E:\\\\data\\\\emoint\\\\tweets_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import keras\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d697470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.utils import *\n",
    "from keras.models import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81906000",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 100 # max number of words in one tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2332eb4",
   "metadata": {},
   "source": [
    "df_list = []\n",
    "\n",
    "for f in os.listdir(data_folder):\n",
    "    if bool(re.search(r'\\.txt$', f)):\n",
    "        f1 = '{}\\{}'.format(data_folder, f)\n",
    "        df = pd.read_csv(f1, \n",
    "            header = 0,\n",
    "            names = ['tweet_id', 'text', 'emotion', 'intensity'],\n",
    "            delimiter = '\\t',\n",
    "            )\n",
    "        df_list.append(df)\n",
    "\n",
    "df_all = pd.concat(df_list)\n",
    "\n",
    "#df_all = df_all.head(1000)\n",
    "\n",
    "df_all.to_excel(\n",
    "    '{}/tweets_all.xlsx'.format(data_folder),\n",
    "    index = False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f808985",
   "metadata": {},
   "source": [
    "# data conversion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6b793",
   "metadata": {},
   "source": [
    "## function to convert text file to text and tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9f8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_tag_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data['label'] = data['emotion'].apply(lambda x: 1 if x == emotion else 0)\n",
    "    texts = data['text'].to_list()\n",
    "    tags = data['label'].to_list()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09973d42",
   "metadata": {},
   "source": [
    "## function to convert text file to text and intensentiy score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edf8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(\n",
    "    x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_file_to_text_and_score_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data = data[data['emotion'] == emotion]\n",
    "    data['intensity'] = data['intensity'].apply(str_to_float)\n",
    "    data = data[data.intensity.notnull()]\n",
    "    texts = data['text'].to_list()\n",
    "    scores = data['intensity'].to_list()\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3376",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824ba35",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9482ac5",
   "metadata": {},
   "source": [
    "## convert text list to a input format of deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daefbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee23928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_input(\n",
    "    texts,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors=\"tf\",\n",
    "        )\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef5720",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb2ce8",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"], method = 'one_hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b004",
   "metadata": {},
   "source": [
    "## function of building the model of text emotion tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dropout_rate = 0.2\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_tagger_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(2, \n",
    "        activation=\"softmax\",\n",
    "         name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf77a",
   "metadata": {},
   "source": [
    "## function of train the model of emotion tagger with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedc6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(\n",
    "    texts,\n",
    "    tags,\n",
    "    tagger_model_path = None,\n",
    "    tagger_model_weight_path = None,\n",
    "    tagger_model_json_path = None,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    dropout_rate = 0.2,\n",
    "    ):\n",
    "    tagger_model = emotion_tagger_model_building(\n",
    "        dropout_rate = dropout_rate,\n",
    "        )\n",
    "    '''\n",
    "    prepare the text input\n",
    "\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    print(numpy.sum(y, axis = 0))\n",
    "    # Fit the model using the train and test datasets.\n",
    "    tagger_model.fit(\n",
    "        [x_ids, x_attention, x_type], y, \n",
    "        validation_split=validation_split, \n",
    "        epochs=epochs)\n",
    "    if tagger_model_path is not None:\n",
    "        tagger_model.save(tagger_model_path)\n",
    "    return tagger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052acfb",
   "metadata": {},
   "source": [
    "# function of intensity score model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee904c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scorer_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):   \n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, \n",
    "        activation=\"sigmoid\",\n",
    "        name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[metrics.mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05725cef",
   "metadata": {},
   "source": [
    "# function of training intensity score model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fea3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scorer(\n",
    "    texts,\n",
    "    scores,\n",
    "    scorer_model_path,\n",
    "    epochs = 10,\n",
    "    validation_split=0.1,\n",
    "    ):\n",
    "    scorer_model = emotion_scorer_model_building()\n",
    "    '''\n",
    "    prepare the text input\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(scores)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    # Fit the model using the train and test datasets.\n",
    "    scorer_model.fit(\n",
    "        [x_ids, x_attention, x_type], \n",
    "        y, \n",
    "        validation_split=0.1, \n",
    "        epochs=epochs)\n",
    "    scorer_model.save(\n",
    "        scorer_model_path,\n",
    "        save_format='h5')\n",
    "    return scorer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d885",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89f6c1",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa43e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_model_fold_cross_validation(\n",
    "    texts,\n",
    "    tags,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_tagger_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('accuracy of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('accuracy of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fcb15",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_model_fold_cross_validation(\n",
    "    texts,\n",
    "    scores,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(scores)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_scorer_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('mse of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('mse of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc963b1b",
   "metadata": {},
   "source": [
    "# training of each emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6adcff",
   "metadata": {},
   "source": [
    "### tagger training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924f00c",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4b4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_tags = convert_file_to_text_and_tag_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2fa4",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18caf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 395s 1s/step - loss: 0.4768 - accuracy: 0.7980\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 420s 2s/step - loss: 0.3343 - accuracy: 0.8628\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 453s 2s/step - loss: 0.2813 - accuracy: 0.8843\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 400s 2s/step - loss: 0.2465 - accuracy: 0.8984\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 414s 2s/step - loss: 0.2041 - accuracy: 0.9192\n",
      "67/67 [==============================] - 99s 1s/step - loss: 0.1777 - accuracy: 0.9338\n",
      "accuracy of the 1-th fold:0.9338061213493347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 410s 1s/step - loss: 0.4576 - accuracy: 0.8076\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 428s 2s/step - loss: 0.3257 - accuracy: 0.8672\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2741 - accuracy: 0.8870\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2195 - accuracy: 0.9114\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 420s 2s/step - loss: 0.2055 - accuracy: 0.9181\n",
      "67/67 [==============================] - 97s 1s/step - loss: 0.1880 - accuracy: 0.9262\n",
      "accuracy of the 2-th fold:0.9262062311172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 405s 1s/step - loss: 0.4545 - accuracy: 0.8094\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 385s 1s/step - loss: 0.3215 - accuracy: 0.8684\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 386s 1s/step - loss: 0.2661 - accuracy: 0.8932\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 1889s 7s/step - loss: 0.2245 - accuracy: 0.9107\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 363s 1s/step - loss: 0.1945 - accuracy: 0.9242\n",
      "67/67 [==============================] - 94s 1s/step - loss: 0.2078 - accuracy: 0.9153\n",
      "accuracy of the 3-th fold:0.9153264164924622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 403s 1s/step - loss: 0.4693 - accuracy: 0.8062\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 405s 2s/step - loss: 0.3260 - accuracy: 0.8630\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 398s 2s/step - loss: 0.2708 - accuracy: 0.8870\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 410s 2s/step - loss: 0.2275 - accuracy: 0.9073\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 429s 2s/step - loss: 0.1922 - accuracy: 0.9230\n",
      "67/67 [==============================] - 98s 1s/step - loss: 0.1959 - accuracy: 0.9267\n",
      "accuracy of the 4-th fold:0.926679253578186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 410s 1s/step - loss: 0.4666 - accuracy: 0.8012\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 400s 1s/step - loss: 0.3354 - accuracy: 0.8585\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 394s 1s/step - loss: 0.2770 - accuracy: 0.8841\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 394s 1s/step - loss: 0.2324 - accuracy: 0.9094\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 395s 1s/step - loss: 0.1946 - accuracy: 0.9215\n",
      "67/67 [==============================] - 98s 1s/step - loss: 0.1707 - accuracy: 0.9333\n",
      "accuracy of the 5-th fold:0.9333018064498901\n",
      "accuracy of 5-fold cross validation:\t0.9270639657974243\n"
     ]
    }
   ],
   "source": [
    "tagger_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_tags,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ddd7",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d532dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 2)\n",
      "[8296. 2275.]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 522s 2s/step - loss: 0.3519 - accuracy: 0.8767 - val_loss: 0.9074 - val_accuracy: 0.4376\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 1763s 6s/step - loss: 0.2426 - accuracy: 0.9091 - val_loss: 1.0522 - val_accuracy: 0.5189\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 2783s 9s/step - loss: 0.1949 - accuracy: 0.9265 - val_loss: 0.8054 - val_accuracy: 0.6607\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 479s 2s/step - loss: 0.1565 - accuracy: 0.9382 - val_loss: 0.7589 - val_accuracy: 0.7060\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 508s 2s/step - loss: 0.1345 - accuracy: 0.9490 - val_loss: 0.7222 - val_accuracy: 0.7561\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 532s 2s/step - loss: 0.1135 - accuracy: 0.9553 - val_loss: 0.9442 - val_accuracy: 0.7051\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 506s 2s/step - loss: 0.0981 - accuracy: 0.9598 - val_loss: 1.5228 - val_accuracy: 0.5888\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 507s 2s/step - loss: 0.0874 - accuracy: 0.9639 - val_loss: 1.1186 - val_accuracy: 0.7004\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 533s 2s/step - loss: 0.0825 - accuracy: 0.9672 - val_loss: 1.1442 - val_accuracy: 0.7202\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 524s 2s/step - loss: 0.0746 - accuracy: 0.9680 - val_loss: 0.9756 - val_accuracy: 0.7675\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_tagger(\n",
    "    texts = fear_texts,\n",
    "    tags = fear_tags,\n",
    "    tagger_model_path = '/data/emoint/{}_tagger_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t8677.124903440475\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b5b2",
   "metadata": {},
   "source": [
    "### scorer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc17c0",
   "metadata": {},
   "source": [
    "#### laod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21372afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_scores = convert_file_to_text_and_score_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea020",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ddf9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 71s 1s/step - loss: 0.0462 - mean_absolute_error: 0.1765\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 59s 1s/step - loss: 0.0271 - mean_absolute_error: 0.1336\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0199 - mean_absolute_error: 0.1125\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 58s 2s/step - loss: 0.0181 - mean_absolute_error: 0.1078\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0169 - mean_absolute_error: 0.1032\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0198 - mean_absolute_error: 0.1131\n",
      "mse of the 1-th fold:0.11308992654085159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 71s 1s/step - loss: 0.0426 - mean_absolute_error: 0.1699\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0243 - mean_absolute_error: 0.1250\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 58s 2s/step - loss: 0.0192 - mean_absolute_error: 0.1110\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.0171 - mean_absolute_error: 0.1037\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.0151 - mean_absolute_error: 0.0971\n",
      "10/10 [==============================] - 16s 1s/step - loss: 0.0172 - mean_absolute_error: 0.1036\n",
      "mse of the 2-th fold:0.10360659658908844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 66s 1s/step - loss: 0.0487 - mean_absolute_error: 0.1784\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 58s 1s/step - loss: 0.0259 - mean_absolute_error: 0.1310\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 58s 2s/step - loss: 0.0208 - mean_absolute_error: 0.1157\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0163 - mean_absolute_error: 0.1034\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 59s 2s/step - loss: 0.0162 - mean_absolute_error: 0.0999\n",
      "10/10 [==============================] - 17s 1s/step - loss: 0.0203 - mean_absolute_error: 0.1123\n",
      "mse of the 3-th fold:0.11227709800004959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 72s 1s/step - loss: 0.0432 - mean_absolute_error: 0.1687\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.0248 - mean_absolute_error: 0.1263\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 58s 2s/step - loss: 0.0194 - mean_absolute_error: 0.1099\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.0193 - mean_absolute_error: 0.1111\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 62s 2s/step - loss: 0.0164 - mean_absolute_error: 0.1016\n",
      "10/10 [==============================] - 18s 1s/step - loss: 0.0197 - mean_absolute_error: 0.1137\n",
      "mse of the 4-th fold:0.11371490359306335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39/39 [==============================] - 66s 1s/step - loss: 0.0471 - mean_absolute_error: 0.1775\n",
      "Epoch 2/5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.0268 - mean_absolute_error: 0.1330\n",
      "Epoch 3/5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.0195 - mean_absolute_error: 0.1101\n",
      "Epoch 4/5\n",
      "39/39 [==============================] - 61s 2s/step - loss: 0.0184 - mean_absolute_error: 0.1064\n",
      "Epoch 5/5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.0162 - mean_absolute_error: 0.1024\n",
      "10/10 [==============================] - 17s 1s/step - loss: 0.0181 - mean_absolute_error: 0.1058\n",
      "mse of the 5-th fold:0.10575933754444122\n",
      "mse of 5-fold cross validation:\t0.10968957245349883\n"
     ]
    }
   ],
   "source": [
    "scorer_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25872af",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7c0bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1530, 100)\n",
      "(1530, 100)\n",
      "(1530, 100)\n",
      "(1530,)\n",
      "Epoch 1/10\n",
      "44/44 [==============================] - 88s 2s/step - loss: 0.0350 - mean_absolute_error: 0.1512 - val_loss: 0.0780 - val_mean_absolute_error: 0.2656\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 75s 2s/step - loss: 0.0202 - mean_absolute_error: 0.1136 - val_loss: 0.0338 - val_mean_absolute_error: 0.1601\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 71s 2s/step - loss: 0.0177 - mean_absolute_error: 0.1063 - val_loss: 0.0512 - val_mean_absolute_error: 0.2075\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.0165 - mean_absolute_error: 0.1031 - val_loss: 0.0936 - val_mean_absolute_error: 0.2942\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 69s 2s/step - loss: 0.0147 - mean_absolute_error: 0.0968 - val_loss: 0.0384 - val_mean_absolute_error: 0.1754\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.0148 - mean_absolute_error: 0.0975 - val_loss: 0.0544 - val_mean_absolute_error: 0.2127\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.0137 - mean_absolute_error: 0.0923 - val_loss: 0.0395 - val_mean_absolute_error: 0.1742\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 69s 2s/step - loss: 0.0135 - mean_absolute_error: 0.0929 - val_loss: 0.0352 - val_mean_absolute_error: 0.1603\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 74s 2s/step - loss: 0.0110 - mean_absolute_error: 0.0836 - val_loss: 0.0638 - val_mean_absolute_error: 0.2320\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 73s 2s/step - loss: 0.0111 - mean_absolute_error: 0.0837 - val_loss: 0.0531 - val_mean_absolute_error: 0.2098\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_scorer(\n",
    "    texts = fear_texts,\n",
    "    scores = fear_scores,\n",
    "    scorer_model_path = '/data/emoint/{}_scorer_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0796c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t752.8080894947052\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
