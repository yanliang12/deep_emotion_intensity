{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"fear\"\n",
    "emotion = \"sadness\"\n",
    "emotion = \"joy\"\n",
    "\n",
    "emotion = \"anger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a3b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"E:\\data\\emoint\"\n",
    "#data_file = \"E:\\\\data\\\\emoint\\\\tweets_sm.xlsx\"\n",
    "data_file = \"E:\\\\data\\\\emoint\\\\tweets_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import keras\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d697470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.utils import *\n",
    "from keras.models import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81906000",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 100 # max number of words in one tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ecfe1",
   "metadata": {},
   "source": [
    "df_list = []\n",
    "\n",
    "for f in os.listdir(data_folder):\n",
    "    if bool(re.search(r'\\.txt$', f)):\n",
    "        f1 = '{}\\{}'.format(data_folder, f)\n",
    "        df = pd.read_csv(f1, \n",
    "            header = 0,\n",
    "            names = ['tweet_id', 'text', 'emotion', 'intensity'],\n",
    "            delimiter = '\\t',\n",
    "            )\n",
    "        df_list.append(df)\n",
    "\n",
    "df_all = pd.concat(df_list)\n",
    "\n",
    "#df_all = df_all.head(1000)\n",
    "\n",
    "df_all.to_excel(\n",
    "    '{}/tweets_all.xlsx'.format(data_folder),\n",
    "    index = False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f808985",
   "metadata": {},
   "source": [
    "# data conversion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6b793",
   "metadata": {},
   "source": [
    "## function to convert text file to text and tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9f8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_tag_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data['label'] = data['emotion'].apply(lambda x: 1 if x == emotion else 0)\n",
    "    texts = data['text'].to_list()\n",
    "    tags = data['label'].to_list()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09973d42",
   "metadata": {},
   "source": [
    "## function to convert text file to text and intensentiy score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edf8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_float(\n",
    "    x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_file_to_text_and_score_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    data = pd.read_excel(data_file)\n",
    "    data = data[data['emotion'] == emotion]\n",
    "    data['intensity'] = data['intensity'].apply(str_to_float)\n",
    "    data = data[data.intensity.notnull()]\n",
    "    texts = data['text'].to_list()\n",
    "    scores = data['intensity'].to_list()\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3376",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824ba35",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9482ac5",
   "metadata": {},
   "source": [
    "## convert text list to a input format of deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daefbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee23928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_input(\n",
    "    texts,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=sequence_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors=\"tf\",\n",
    "        )\n",
    "    # Convert batch of encoded features to numpy array.\n",
    "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "    return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef5720",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb2ce8",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"], method = 'one_hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b004",
   "metadata": {},
   "source": [
    "## function of building the model of text emotion tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dropout_rate = 0.2\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_tagger_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(2, \n",
    "        activation=\"softmax\",\n",
    "         name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf77a",
   "metadata": {},
   "source": [
    "## function of train the model of emotion tagger with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedc6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(\n",
    "    texts,\n",
    "    tags,\n",
    "    tagger_model_path = None,\n",
    "    tagger_model_weight_path = None,\n",
    "    tagger_model_json_path = None,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    dropout_rate = 0.2,\n",
    "    ):\n",
    "    tagger_model = emotion_tagger_model_building(\n",
    "        dropout_rate = dropout_rate,\n",
    "        )\n",
    "    '''\n",
    "    prepare the text input\n",
    "\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    print(numpy.sum(y, axis = 0))\n",
    "    # Fit the model using the train and test datasets.\n",
    "    tagger_model.fit(\n",
    "        [x_ids, x_attention, x_type], y, \n",
    "        validation_split=validation_split, \n",
    "        epochs=epochs)\n",
    "    if tagger_model_path is not None:\n",
    "        tagger_model.save(tagger_model_path)\n",
    "    return tagger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052acfb",
   "metadata": {},
   "source": [
    "# function of intensity score model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee904c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scorer_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):   \n",
    "    ### input layers\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    # Attention masks indicates to the model which tokens should be attended to.\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "    # Token type ids are binary masks identifying different sequences in the model.\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
    "    )\n",
    "    # Loading pretrained BERT model.\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "    bert_model.trainable = False\n",
    "    ###load the bert model \n",
    "    bert_output = bert_model(\n",
    "        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    "    )\n",
    "    sequence_output = bert_output.last_hidden_state   \n",
    "    x = layers.Dropout(dropout_rate)(sequence_output)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, \n",
    "        activation=\"sigmoid\",\n",
    "        name=\"predictions\")(x)\n",
    "    model = keras.Model([\n",
    "        input_ids,\n",
    "        attention_masks,\n",
    "        token_type_ids,\n",
    "    ], predictions)\n",
    "    model.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[metrics.mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05725cef",
   "metadata": {},
   "source": [
    "# function of training intensity score model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fea3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scorer(\n",
    "    texts,\n",
    "    scores,\n",
    "    scorer_model_path,\n",
    "    epochs = 10,\n",
    "    validation_split=0.1,\n",
    "    ):\n",
    "    scorer_model = emotion_scorer_model_building()\n",
    "    '''\n",
    "    prepare the text input\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(scores)\n",
    "    print(x_ids.shape)\n",
    "    print(x_attention.shape)\n",
    "    print(x_type.shape)\n",
    "    print(y.shape)\n",
    "    # Fit the model using the train and test datasets.\n",
    "    scorer_model.fit(\n",
    "        [x_ids, x_attention, x_type], \n",
    "        y, \n",
    "        validation_split=0.1, \n",
    "        epochs=epochs)\n",
    "    scorer_model.save(\n",
    "        scorer_model_path,\n",
    "        save_format='h5')\n",
    "    return scorer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d885",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89f6c1",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa43e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_model_fold_cross_validation(\n",
    "    texts,\n",
    "    tags,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_tagger_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('accuracy of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('accuracy of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fcb15",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_model_fold_cross_validation(\n",
    "    texts,\n",
    "    scores,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x_ids, x_attention, x_type = texts_to_input(texts)\n",
    "    y = numpy.array(scores)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x_ids, y):\n",
    "        tagger_model = emotion_scorer_model_building()\n",
    "        tagger_model.fit(\n",
    "            [x_ids[train], x_attention[train], x_type[train]],\n",
    "            y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 1)\n",
    "        scores = tagger_model.evaluate(\n",
    "            [x_ids[test], x_attention[test], x_type[test]],\n",
    "            y[test], \n",
    "            verbose = 1)\n",
    "        print('mse of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('mse of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc963b1b",
   "metadata": {},
   "source": [
    "# training of each emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6adcff",
   "metadata": {},
   "source": [
    "### tagger training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924f00c",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4b4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_tags = convert_file_to_text_and_tag_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2fa4",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18caf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 449s 2s/step - loss: 0.4432 - accuracy: 0.8049\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 477s 2s/step - loss: 0.3025 - accuracy: 0.8744\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 408s 2s/step - loss: 0.2466 - accuracy: 0.9023\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 425s 2s/step - loss: 0.2065 - accuracy: 0.9176\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 490s 2s/step - loss: 0.1858 - accuracy: 0.9243\n",
      "67/67 [==============================] - 116s 2s/step - loss: 0.1657 - accuracy: 0.9390\n",
      "accuracy of the 1-th fold:0.9390071034431458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 445s 2s/step - loss: 0.4531 - accuracy: 0.8013\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 424s 2s/step - loss: 0.2975 - accuracy: 0.8786\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 428s 2s/step - loss: 0.2492 - accuracy: 0.9022\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 422s 2s/step - loss: 0.2148 - accuracy: 0.9163\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 546s 2s/step - loss: 0.1894 - accuracy: 0.9261\n",
      "67/67 [==============================] - 127s 2s/step - loss: 0.1659 - accuracy: 0.9324\n",
      "accuracy of the 2-th fold:0.9323557019233704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 580s 2s/step - loss: 0.4421 - accuracy: 0.8037\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 1942s 7s/step - loss: 0.2986 - accuracy: 0.8777\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 537s 2s/step - loss: 0.2483 - accuracy: 0.9008\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 580s 2s/step - loss: 0.2171 - accuracy: 0.9164\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 577s 2s/step - loss: 0.1857 - accuracy: 0.9269\n",
      "67/67 [==============================] - 110s 2s/step - loss: 0.1629 - accuracy: 0.9390\n",
      "accuracy of the 3-th fold:0.9389782547950745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 486s 2s/step - loss: 0.4552 - accuracy: 0.7925\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 510s 2s/step - loss: 0.3094 - accuracy: 0.8716\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 580s 2s/step - loss: 0.2466 - accuracy: 0.9001\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 570s 2s/step - loss: 0.2122 - accuracy: 0.9175\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 569s 2s/step - loss: 0.1811 - accuracy: 0.9275\n",
      "67/67 [==============================] - 114s 2s/step - loss: 0.2056 - accuracy: 0.9257\n",
      "accuracy of the 4-th fold:0.925733208656311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 450s 2s/step - loss: 0.4368 - accuracy: 0.8131\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 1039s 4s/step - loss: 0.2958 - accuracy: 0.8839\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 3477s 13s/step - loss: 0.2408 - accuracy: 0.9055\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 551s 2s/step - loss: 0.2165 - accuracy: 0.9149\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 597s 2s/step - loss: 0.1819 - accuracy: 0.9291\n",
      "67/67 [==============================] - 111s 2s/step - loss: 0.1543 - accuracy: 0.9395\n",
      "accuracy of the 5-th fold:0.939451277256012\n",
      "accuracy of 5-fold cross validation:\t0.9351051092147827\n"
     ]
    }
   ],
   "source": [
    "tagger_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_tags,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ddd7",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d532dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 100)\n",
      "(10571, 2)\n",
      "[8031. 2540.]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 637s 2s/step - loss: 0.4337 - accuracy: 0.8071 - val_loss: 0.2826 - val_accuracy: 0.8875\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 669s 2s/step - loss: 0.2888 - accuracy: 0.8824 - val_loss: 0.2391 - val_accuracy: 0.9149\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 539s 2s/step - loss: 0.2348 - accuracy: 0.9073 - val_loss: 0.2294 - val_accuracy: 0.9178\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 534s 2s/step - loss: 0.1940 - accuracy: 0.9211 - val_loss: 0.1923 - val_accuracy: 0.9329\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 700s 2s/step - loss: 0.1701 - accuracy: 0.9338 - val_loss: 0.2113 - val_accuracy: 0.9159\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 631s 2s/step - loss: 0.1453 - accuracy: 0.9454 - val_loss: 0.2365 - val_accuracy: 0.9130\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 568s 2s/step - loss: 0.1302 - accuracy: 0.9491 - val_loss: 0.2201 - val_accuracy: 0.9168\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 698s 2s/step - loss: 0.1128 - accuracy: 0.9577 - val_loss: 0.1878 - val_accuracy: 0.9367\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 278s 934ms/step - loss: 0.1025 - accuracy: 0.9601 - val_loss: 0.1835 - val_accuracy: 0.9282\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 265s 891ms/step - loss: 0.0876 - accuracy: 0.9645 - val_loss: 0.2533 - val_accuracy: 0.9234\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_tagger(\n",
    "    texts = fear_texts,\n",
    "    tags = fear_tags,\n",
    "    tagger_model_path = '/data/emoint/{}_tagger_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t5546.219535112381\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b5b2",
   "metadata": {},
   "source": [
    "### scorer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc17c0",
   "metadata": {},
   "source": [
    "#### laod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21372afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_scores = convert_file_to_text_and_score_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea020",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ddf9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "43/43 [==============================] - 45s 799ms/step - loss: 0.0329 - mean_absolute_error: 0.1419\n",
      "Epoch 2/5\n",
      "43/43 [==============================] - 34s 796ms/step - loss: 0.0188 - mean_absolute_error: 0.1100\n",
      "Epoch 3/5\n",
      "43/43 [==============================] - 34s 795ms/step - loss: 0.0166 - mean_absolute_error: 0.1027\n",
      "Epoch 4/5\n",
      "43/43 [==============================] - 34s 798ms/step - loss: 0.0130 - mean_absolute_error: 0.0899\n",
      "Epoch 5/5\n",
      "43/43 [==============================] - 28s 640ms/step - loss: 0.0120 - mean_absolute_error: 0.0863\n",
      "11/11 [==============================] - 6s 275ms/step - loss: 0.0144 - mean_absolute_error: 0.0947\n",
      "mse of the 1-th fold:0.0946657657623291\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"intermediate\" (type TFBertIntermediate).\n\nfailed to allocate memory [Op:Mul]\n\nCall arguments received:\n  â€¢ hidden_states=tf.Tensor(shape=(3, 5, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscorer_model_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfear_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfear_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mscorer_model_fold_cross_validation\u001b[1;34m(texts, scores, n_splits)\u001b[0m\n\u001b[0;32m     15\u001b[0m fold_no \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m kfold\u001b[38;5;241m.\u001b[39msplit(x_ids, y):\n\u001b[1;32m---> 17\u001b[0m     tagger_model \u001b[38;5;241m=\u001b[39m \u001b[43memotion_scorer_model_building\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     tagger_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     19\u001b[0m         [x_ids[train], x_attention[train], x_type[train]],\n\u001b[0;32m     20\u001b[0m         y[train], \n\u001b[0;32m     21\u001b[0m         epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     22\u001b[0m         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m     scores \u001b[38;5;241m=\u001b[39m tagger_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m     24\u001b[0m         [x_ids[test], x_attention[test], x_type[test]],\n\u001b[0;32m     25\u001b[0m         y[test], \n\u001b[0;32m     26\u001b[0m         verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36memotion_scorer_model_building\u001b[1;34m(embedding_dim, filters, kernel_size, dropout_rate, sequence_length)\u001b[0m\n\u001b[0;32m     17\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[0;32m     18\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(sequence_length,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Loading pretrained BERT model.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Freeze the BERT model to reuse the pretrained features without modifying them.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m bert_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1803\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m         model(model\u001b[38;5;241m.\u001b[39mdummy_inputs)  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1803\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_archive_file), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_archive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# 'by_name' allow us to do transfer learning by skipping/adding layers\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;66;03m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m main_input \u001b[38;5;241m=\u001b[39m fn_args_and_kwargs\u001b[38;5;241m.\u001b[39mpop(main_input_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    382\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, main_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1090\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1069\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1090\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:383\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m main_input \u001b[38;5;241m=\u001b[39m fn_args_and_kwargs\u001b[38;5;241m.\u001b[39mpop(main_input_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    382\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, main_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:850\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    848\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[1;32m--> 850\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(hidden_states\u001b[38;5;241m=\u001b[39msequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:541\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    537\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    539\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 541\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:497\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    494\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    495\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 497\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_output(\n\u001b[0;32m    499\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mintermediate_output, input_tensor\u001b[38;5;241m=\u001b[39mattention_output, training\u001b[38;5;241m=\u001b[39mtraining\n\u001b[0;32m    500\u001b[0m )\n\u001b[0;32m    501\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\gaoyu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:400\u001b[0m, in \u001b[0;36mTFBertIntermediate.call\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: tf\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 400\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"intermediate\" (type TFBertIntermediate).\n\nfailed to allocate memory [Op:Mul]\n\nCall arguments received:\n  â€¢ hidden_states=tf.Tensor(shape=(3, 5, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "scorer_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25872af",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_scorer(\n",
    "    texts = fear_texts,\n",
    "    scores = fear_scores,\n",
    "    scorer_model_path = '/data/emoint/{}_scorer_bert.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
