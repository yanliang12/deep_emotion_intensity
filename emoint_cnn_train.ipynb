{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = \"anger\"\n",
    "emotion = \"fear\"\n",
    "emotion = \"sadness\"\n",
    "emotion = \"joy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4c3eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 14:05:10.193209: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-03 14:05:10.193251: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import keras\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "from keras.utils import *\n",
    "from keras.models import *\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3401e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad135e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/03 14:05:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/03 14:05:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local\")\n",
    "sqlContext = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81906000",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 100 #max number of words in one tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8c8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2float(input):\n",
    "    try:\n",
    "        return float(input)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "udf_str2float = udf(str2float, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f808985",
   "metadata": {},
   "source": [
    "# data conversion functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6b793",
   "metadata": {},
   "source": [
    "## function to convert text file to text and tag list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9f8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_tag_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    schema = StructType()\\\n",
    "        .add(\"tweet_id\",StringType(),True)\\\n",
    "        .add(\"text\",StringType(),True)\\\n",
    "        .add(\"emotion\",StringType(),True)\\\n",
    "        .add(\"intensity\",FloatType(),True)\n",
    "    train = sqlContext.read.format('csv')\\\n",
    "        .options(delimiter='\\t')\\\n",
    "        .schema(schema)\\\n",
    "        .load(data_file)\\\n",
    "        .withColumn(\"intensity\", udf_str2float(\"intensity\"))\n",
    "    train.registerTempTable(\"train\")\n",
    "    train_list = sqlContext.sql(u\"\"\"\n",
    "        SELECT *, \n",
    "        CASE \n",
    "            WHEN emotion = '%s' THEN 1\n",
    "            ELSE 0\n",
    "        END AS label\n",
    "        FROM train\n",
    "        WHERE emotion IS NOT NULL\n",
    "        \"\"\"%(emotion_tag)).collect()\n",
    "    texts = [r.text for r in train_list]\n",
    "    tags = [r.label for r in train_list]\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09973d42",
   "metadata": {},
   "source": [
    "## function to convert text file to text and intensentiy score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edf8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_text_and_score_list(\n",
    "    emotion_tag,\n",
    "    data_file,\n",
    "    ):\n",
    "    udf_str2float = udf(str2float, FloatType())\n",
    "    schema = StructType()\\\n",
    "        .add(\"tweet_id\",StringType(),True)\\\n",
    "        .add(\"text\",StringType(),True)\\\n",
    "        .add(\"emotion\",StringType(),True)\\\n",
    "        .add(\"intensity\",FloatType(),True)\n",
    "    train = sqlContext.read.format('csv')\\\n",
    "        .options(delimiter='\\t')\\\n",
    "        .schema(schema)\\\n",
    "        .load(data_file)\\\n",
    "        .withColumn(\"intensity\", udf_str2float(\"intensity\"))\n",
    "    train.registerTempTable(\"train\")\n",
    "    train_list = sqlContext.sql(u\"\"\"\n",
    "        SELECT *\n",
    "        FROM train\n",
    "        WHERE emotion = '%s' AND intensity IS NOT NULL\n",
    "        \"\"\"%(emotion_tag)).collect()\n",
    "    texts = [r.text for r in train_list]\n",
    "    scores = [r.intensity for r in train_list]\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3376",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824ba35",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9482ac5",
   "metadata": {},
   "source": [
    "## convert text list to a input format of deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee23928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_input(\n",
    "    texts,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    word_id_sequence = map(lambda x: \n",
    "        keras.preprocessing.text.one_hot(x, n=max_features), \n",
    "        texts)\n",
    "    word_id_sequence = list(word_id_sequence)\n",
    "    #print(word_id_sequence)\n",
    "    x = numpy.array(word_id_sequence)\n",
    "    x = keras.preprocessing.sequence.pad_sequences(\n",
    "        x, padding=\"post\",\n",
    "        maxlen=sequence_length,\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05d07a",
   "metadata": {},
   "source": [
    "texts_to_input([\"Never dull moment here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b004",
   "metadata": {},
   "source": [
    "## function of building the model of text emotion tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aad8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dropout_rate = 0.2\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_tagger_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = keras.Input(shape=(sequence_length, ))\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(2, \n",
    "        activation=\"softmax\",\n",
    "         name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, predictions)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf77a",
   "metadata": {},
   "source": [
    "## function of train the model of emotion tagger with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aedc6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(texts,\n",
    "    tags,\n",
    "    tagger_model_path = None,\n",
    "    tagger_model_weight_path = None,\n",
    "    tagger_model_json_path = None,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    dropout_rate = 0.2,\n",
    "    ):\n",
    "    tagger_model = emotion_tagger_model_building(\n",
    "        dropout_rate = dropout_rate,\n",
    "        )\n",
    "    '''\n",
    "    prepare the text input\n",
    "\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    print(x.shape, y.shape)\n",
    "    print(numpy.sum(y, axis = 0))\n",
    "    # Fit the model using the train and test datasets.\n",
    "    tagger_model.fit(x, y, \n",
    "        validation_split=validation_split, \n",
    "        epochs=epochs)\n",
    "    # serialize model to JSON\n",
    "    if tagger_model_json_path is not None:\n",
    "        model_json = tagger_model.to_json()\n",
    "        with open(tagger_model_json_path, 'w+') as json_file:\n",
    "            json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    if tagger_model_weight_path is not None:\n",
    "        tagger_model.save_weights(tagger_model_weight_path)\n",
    "    if tagger_model_path is not None:\n",
    "        tagger_model.save(tagger_model_path)\n",
    "    return tagger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052acfb",
   "metadata": {},
   "source": [
    "# function of intensity score model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee904c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_scorer_model_building(\n",
    "    embedding_dim = 300,\n",
    "    filters = 128,\n",
    "    kernel_size = 2,\n",
    "    dropout_rate = 0.2,\n",
    "    sequence_length = 100,\n",
    "    ):\n",
    "    # A integer input for vocab indices.\n",
    "    inputs = keras.Input(shape=(sequence_length, ))\n",
    "    # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "    # 'embedding_dim'.\n",
    "    x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(filters, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, \n",
    "        activation=\"sigmoid\",\n",
    "        name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, predictions)\n",
    "    model.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[metrics.mean_absolute_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05725cef",
   "metadata": {},
   "source": [
    "# function of training intensity score model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fea3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scorer(texts,\n",
    "    scores,\n",
    "    scorer_model_path,\n",
    "    epochs = 10,\n",
    "    validation_split=0.1,\n",
    "    ):\n",
    "    scorer_model = emotion_scorer_model_building()\n",
    "    '''\n",
    "    prepare the text input\n",
    "    texts = [\n",
    "        \"i feel so fear\",\n",
    "        \"nothing is wrong\"\n",
    "        ]\n",
    "    '''\n",
    "    x = texts_to_input(texts)\n",
    "    '''\n",
    "    prepare the output\n",
    "    '''\n",
    "    y = numpy.array(scores)\n",
    "    print(x.shape, y.shape)\n",
    "    # Fit the model using the train and test datasets.\n",
    "    scorer_model.fit(x, y, \n",
    "        validation_split=0.1, \n",
    "        epochs=epochs)\n",
    "    scorer_model.save(\n",
    "        scorer_model_path,\n",
    "        save_format='h5')\n",
    "    return scorer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7d885",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89f6c1",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa43e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_model_fold_cross_validation(\n",
    "    texts,\n",
    "    tags,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x = texts_to_input(texts)\n",
    "    y = numpy.array(tags)\n",
    "    y = to_categorical(y)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x, y):\n",
    "        tagger_model = emotion_tagger_model_building()\n",
    "        tagger_model.fit(\n",
    "            x[train], y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 0)\n",
    "        scores = tagger_model.evaluate(\n",
    "            x[test], y[test], \n",
    "            verbose = 0)\n",
    "        print('accuracy of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('accuracy of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252fcb15",
   "metadata": {},
   "source": [
    "### K-fold cross validation function of scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_model_fold_cross_validation(\n",
    "    texts,\n",
    "    scores,\n",
    "    n_splits = 5,\n",
    "    ):\n",
    "    #convert data to numpy arrays\n",
    "    x = texts_to_input(texts)\n",
    "    y = numpy.array(scores)\n",
    "    #make the k folds\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    #\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    #\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x, y):\n",
    "        tagger_model = emotion_scorer_model_building()\n",
    "        tagger_model.fit(\n",
    "            x[train], y[train], \n",
    "            epochs = 5,\n",
    "            verbose = 0)\n",
    "        scores = tagger_model.evaluate(\n",
    "            x[test], y[test], \n",
    "            verbose = 0)\n",
    "        print('mse of the {}-th fold:{}'.format(\n",
    "            fold_no,\n",
    "            scores[1]))\n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "    ###\n",
    "    acc_10_fold_cross_validation = numpy.mean(numpy.array(acc_per_fold))\n",
    "    print('mse of {}-fold cross validation:\\t{}'.format(\n",
    "        n_splits,\n",
    "        acc_10_fold_cross_validation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc963b1b",
   "metadata": {},
   "source": [
    "# training of each emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6adcff",
   "metadata": {},
   "source": [
    "### tagger training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924f00c",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4b4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fear_texts, fear_tags = convert_file_to_text_and_tag_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = \"/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c2fa4",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18caf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13110/4256197761.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = numpy.array(word_id_sequence)\n",
      "2022-04-03 14:05:19.565722: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-03 14:05:19.565765: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-03 14:05:19.565782: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (07b0cb0e5eeb): /proc/driver/nvidia/version does not exist\n",
      "2022-04-03 14:05:19.565950: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the 1-th fold:0.9482323527336121\n",
      "accuracy of the 2-th fold:0.9318181872367859\n",
      "accuracy of the 3-th fold:0.9558081030845642\n",
      "accuracy of the 4-th fold:0.9280303120613098\n",
      "accuracy of the 5-th fold:0.9482323527336121\n",
      "accuracy of 5-fold cross validation:\t0.9424242615699768\n"
     ]
    }
   ],
   "source": [
    "tagger_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_tags,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503ddd7",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d532dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3960, 100) (3960, 2)\n",
      "[3058.  902.]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13110/4256197761.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = numpy.array(word_id_sequence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 5s 40ms/step - loss: 0.5016 - accuracy: 0.7800 - val_loss: 0.6485 - val_accuracy: 0.6768\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.1975 - accuracy: 0.9248 - val_loss: 0.3744 - val_accuracy: 0.8939\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 5s 42ms/step - loss: 0.0476 - accuracy: 0.9823 - val_loss: 0.3563 - val_accuracy: 0.8990\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.5716 - val_accuracy: 0.8712\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0074 - accuracy: 0.9992 - val_loss: 0.3966 - val_accuracy: 0.8965\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.6424 - val_accuracy: 0.8712\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.4173 - val_accuracy: 0.9015\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.9178 - val_accuracy: 0.8384\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 4s 39ms/step - loss: 0.0071 - accuracy: 0.9992 - val_loss: 0.7234 - val_accuracy: 0.8561\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 4s 40ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 0.4662 - val_accuracy: 0.8965\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_tagger(\n",
    "    texts = fear_texts,\n",
    "    tags = fear_tags,\n",
    "    tagger_model_path = '/data/emoint/{}_tagger.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "968de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t45.8455126285553\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b5b2",
   "metadata": {},
   "source": [
    "### scorer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc17c0",
   "metadata": {},
   "source": [
    "#### laod data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21372afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fear_texts, fear_scores = convert_file_to_text_and_score_list(\n",
    "    emotion_tag = emotion,\n",
    "    data_file = \"/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea020",
   "metadata": {},
   "source": [
    "#### 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ddf9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13110/4256197761.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = numpy.array(word_id_sequence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse of the 1-th fold:0.12508437037467957\n",
      "mse of the 2-th fold:0.1372532695531845\n",
      "mse of the 3-th fold:0.13800092041492462\n",
      "mse of the 4-th fold:0.1397487074136734\n",
      "mse of the 5-th fold:0.14118005335330963\n",
      "mse of 5-fold cross validation:\t0.13625346422195433\n"
     ]
    }
   ],
   "source": [
    "scorer_model_fold_cross_validation(\n",
    "    fear_texts,\n",
    "    fear_scores,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25872af",
   "metadata": {},
   "source": [
    "#### train single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7c0bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(902, 100) (902,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13110/4256197761.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = numpy.array(word_id_sequence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 2s 44ms/step - loss: 0.0396 - mean_absolute_error: 0.1632 - val_loss: 0.0653 - val_mean_absolute_error: 0.2180\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.0346 - mean_absolute_error: 0.1521 - val_loss: 0.0547 - val_mean_absolute_error: 0.2000\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.0199 - mean_absolute_error: 0.1103 - val_loss: 0.0506 - val_mean_absolute_error: 0.1723\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.0089 - mean_absolute_error: 0.0740 - val_loss: 0.0462 - val_mean_absolute_error: 0.1674\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 0.0048 - mean_absolute_error: 0.0550 - val_loss: 0.0439 - val_mean_absolute_error: 0.1656\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 0.0042 - mean_absolute_error: 0.0504 - val_loss: 0.0435 - val_mean_absolute_error: 0.1650\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 0.0034 - mean_absolute_error: 0.0453 - val_loss: 0.0429 - val_mean_absolute_error: 0.1633\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.0032 - mean_absolute_error: 0.0445 - val_loss: 0.0428 - val_mean_absolute_error: 0.1626\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.0026 - mean_absolute_error: 0.0396 - val_loss: 0.0426 - val_mean_absolute_error: 0.1623\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 0.0023 - mean_absolute_error: 0.0376 - val_loss: 0.0417 - val_mean_absolute_error: 0.1604\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "fear_tagger = train_scorer(\n",
    "    texts = fear_texts,\n",
    "    scores = fear_scores,\n",
    "    scorer_model_path = '/data/emoint/{}_scorer.h5'.format(emotion),\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0796c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time of scorer:\t12.015078783035278\n"
     ]
    }
   ],
   "source": [
    "print('training time of scorer:\\t{}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
